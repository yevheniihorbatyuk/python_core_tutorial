"""
–ú–æ–¥—É–ª—å 4.4: Data Processing —Ç–∞ ETL - Professional Edition
===========================================================

–¶–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∏–π –º–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö.
–í–∫–ª—é—á–∞—î –ø—Ä–∏–∫–ª–∞–¥–∏ –∑:
- –û–±—Ä–æ–±–∫–∞ CSV —Ç–∞ JSON —Ñ–∞–π–ª—ñ–≤ —É production
- ETL (Extract-Transform-Load) pipeline
- Memory-efficient –æ–±—Ä–æ–±–∫–∞ –≤–µ–ª–∏–∫–∏—Ö —Ñ–∞–π–ª—ñ–≤
- –ë–∞–ª–∞–Ω—Å—É–≤–∞–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ —Ç–∞ —à–≤–∏–¥–∫–æ—Å—Ç—ñ
- –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ (multi-threading)
- –ö–µ—à—É–≤–∞–Ω–Ω—è —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è
- –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ —Ç–∞ –ª–æ–≥—É–≤–∞–Ω–Ω—è

–ö–ª—é—á–æ–≤—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó:
- Streaming –æ–±—Ä–æ–±–∫–∞ (–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤—Å–µ –≤ –ø–∞–º'—è—Ç—å)
- Chunks –æ–±—Ä–æ–±–∫–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å—É
- Type hints —Ç–∞ validation –Ω–∞ –≤—Å—ñ—Ö –µ—Ç–∞–ø–∞—Ö
- –ú–µ—Ç—Ä–∏–∫–∏ —Ç–∞ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –æ–±—Ä–æ–±–∫–∏
- Graceful degradation –ø—Ä–∏ –ø–æ–º–∏–ª–∫–∞—Ö

–¶–µ –≤–µ—Ä—Å—ñ—è –¥–ª—è Senior engineers - production-ready.
"""

import csv
import json
import os
import time
import logging
from typing import List, Dict, Optional, Generator, Tuple
from dataclasses import dataclass, asdict, field
from collections import defaultdict
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

print("=" * 70)
print("–ú–û–î–£–õ–¨ 4.4: DATA PROCESSING –¢–ê ETL - PROFESSIONAL EDITION")
print("=" * 70)

# ============================================================================
# 1. STREAMING CSV –û–ë–†–û–ë–ö–ê
# ============================================================================

print("\n" + "=" * 70)
print("PART 1: STREAMING CSV –û–ë–†–û–ë–ö–ê - MEMORY EFFICIENT")
print("=" * 70)

@dataclass
class CSVMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è CSV –æ–±—Ä–æ–±–∫–∏."""
    total_rows: int = 0
    processed_rows: int = 0
    skipped_rows: int = 0
    error_rows: int = 0
    processing_time: float = 0.0
    throughput: float = 0.0

class CSVProcessor:
    """Production-ready CSV –æ–±—Ä–æ–±–Ω–∏–∫ –∑ streaming —Ç–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è–º."""

    def __init__(self, chunk_size: int = 1000):
        self.chunk_size = chunk_size
        self.metrics = CSVMetrics()
        self._cache = {}

    def process_csv_streaming(
        self, filepath: str, row_processor
    ) -> CSVMetrics:
        """–û–±—Ä–æ–±–ª—è—î CSV —Ñ–∞–π–ª streaming —Å–ø–æ—Å–æ–±–æ–º (chunks)."""
        start_time = time.time()
        chunk = []

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)

                for row in reader:
                    self.metrics.total_rows += 1

                    try:
                        processed = row_processor(row)
                        if processed:
                            chunk.append(processed)
                            self.metrics.processed_rows += 1
                        else:
                            self.metrics.skipped_rows += 1

                    except Exception as e:
                        logger.warning(f"Row error: {e}")
                        self.metrics.error_rows += 1

                    # –ë–∞—Ç—á –æ–±—Ä–æ–±–∫–∞
                    if len(chunk) >= self.chunk_size:
                        self._process_batch(chunk)
                        chunk = []

                # –û–±—Ä–æ–±–∏—Ç–∏ –∑–∞–ª–∏—à–æ–∫
                if chunk:
                    self._process_batch(chunk)

        except IOError as e:
            logger.error(f"File read error: {e}")

        self.metrics.processing_time = time.time() - start_time
        self.metrics.throughput = (
            self.metrics.processed_rows / self.metrics.processing_time
            if self.metrics.processing_time > 0 else 0
        )

        return self.metrics

    def _process_batch(self, batch: List[Dict]) -> None:
        """–û–±—Ä–æ–±–ª—è—î –±–∞—Ç—á (–º–æ–∂–Ω–∞ —Ä–æ–∑—à–∏—Ä–∏—Ç–∏ –¥–ª—è DB –∑–∞–ø–∏—Å—É, etc)."""
        # Placeholder –¥–ª—è –±–∞—Ç—á –æ–±—Ä–æ–±–∫–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, DB insert)
        pass

# –ì–µ–Ω–µ—Ä—É—î–º–æ —Ç–µ—Å—Ç–æ–≤–∏–π CSV —Ñ–∞–π–ª
test_csv = "sales_data.csv"
with open(test_csv, 'w', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=['id', 'product', 'quantity', 'price', 'date'])
    writer.writeheader()
    for i in range(100):
        writer.writerow({
            'id': i,
            'product': f'Product_{i % 10}',
            'quantity': 10 + (i % 50),
            'price': 99.99 + i,
            'date': '2024-01-15'
        })

print("\n1. STREAMING CSV –û–ë–†–û–ë–ö–ê:")
print("-" * 70)

def validate_sales_row(row: Dict) -> Optional[Dict]:
    """–í–∞–ª—ñ–¥—É—î —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î —Ä—è–¥–æ–∫ –ø—Ä–æ–¥–∞–∂—É."""
    try:
        quantity = int(row['quantity'])
        price = float(row['price'])

        if quantity <= 0 or price <= 0:
            return None

        return {
            'product': row['product'].strip(),
            'quantity': quantity,
            'price': price,
            'total': quantity * price
        }
    except (ValueError, KeyError):
        return None

processor = CSVProcessor(chunk_size=25)
metrics = processor.process_csv_streaming(test_csv, validate_sales_row)

print(f"‚úÖ CSV –æ–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
print(f"  Total rows: {metrics.total_rows}")
print(f"  Processed: {metrics.processed_rows}")
print(f"  Skipped: {metrics.skipped_rows}")
print(f"  Errors: {metrics.error_rows}")
print(f"  Time: {metrics.processing_time:.3f}s")
print(f"  Throughput: {metrics.throughput:.0f} rows/sec")

print()

# ============================================================================
# 2. JSON STREAMING –û–ë–†–û–ë–ö–ê
# ============================================================================

print("\n" + "=" * 70)
print("PART 2: JSON STREAMING - HANDLING LARGE FILES")
print("=" * 70)

# –ì–µ–Ω–µ—Ä—É—î–º–æ —Ç–µ—Å—Ç–æ–≤–∏–π JSON —Ñ–∞–π–ª (JSONL - –æ–¥–Ω–∞ JSON –æ–±'—î–∫—Ç –Ω–∞ —Ä—è–¥–æ–∫)
test_jsonl = "events.jsonl"
with open(test_jsonl, 'w', encoding='utf-8') as f:
    for i in range(50):
        event = {
            'id': i,
            'user_id': i % 10,
            'event_type': ['page_view', 'click', 'purchase'][i % 3],
            'timestamp': '2024-01-15T10:00:00Z',
            'value': 10.0 + (i % 100)
        }
        f.write(json.dumps(event) + '\n')

def stream_jsonl_events(filepath: str) -> Generator[Dict, None, None]:
    """Streaming –æ–±—Ä–æ–±–∫–∞ JSONL —Ñ–∞–π–ª—É."""
    with open(filepath, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                yield json.loads(line.strip())
            except json.JSONDecodeError as e:
                logger.warning(f"Line {line_num} JSON error: {e}")
                continue

print("\n1. JSONL STREAMING –û–ë–†–û–ë–ö–ê:")
print("-" * 70)

event_stats = defaultdict(lambda: {'count': 0, 'total_value': 0})
total_events = 0

for event in stream_jsonl_events(test_jsonl):
    total_events += 1
    event_type = event.get('event_type', 'unknown')
    event_stats[event_type]['count'] += 1
    event_stats[event_type]['total_value'] += event.get('value', 0)

print(f"‚úÖ –û–±—Ä–æ–±–ª–µ–Ω–æ {total_events} –µ–≤–µ–Ω—Ç—ñ–≤")
print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º:")
for event_type in sorted(event_stats.keys()):
    stats = event_stats[event_type]
    print(f"  {event_type:12s}: {stats['count']:3d} events, ${stats['total_value']:8.2f}")

print()

# ============================================================================
# 3. –î–ï–î–£¬≠–ü–õ–Ü¬≠–ö–ê¬≠–¶–Ü–Ø —Ç–∞ –û–ß–ò–©–ï–ù–ù–Ø
# ============================================================================

print("\n" + "=" * 70)
print("PART 3: –î–ï–î–£–ë–õ–Ü–ö–ê–¶–Ü–Ø –¢–ê –ù–û–†–ú–ê–õ–Ü–ó–ê–¶–Ü–Ø")
print("=" * 70)

@dataclass
class UserRecord:
    """–ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ü—å–∫–∏–π –∑–∞–ø–∏—Å."""
    user_id: str
    email: str
    name: str
    country: str

    def get_hash(self) -> str:
        """–û—Ç—Ä–∏–º—É—î —Ö–µ—à –¥–ª—è –¥–µ–¥—É–±–ª—ñ–∫–∞—Ü—ñ—ó."""
        content = f"{self.user_id}|{self.email}|{self.name}|{self.country}"
        return hashlib.md5(content.encode()).hexdigest()

class DuplicateEliminator:
    """–í–∏–¥–∞–ª—è—î –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑ –ø–æ—Ç–æ–∫—É –¥–∞–Ω–∏—Ö."""

    def __init__(self):
        self.seen_hashes = set()
        self.duplicate_count = 0

    def add_record(self, record: UserRecord) -> Optional[UserRecord]:
        """–î–æ–¥–∞—î –∑–∞–ø–∏—Å, –ø–æ–≤–µ—Ä—Ç–∞—î None —è–∫—â–æ –¥—É–±–ª—ñ–∫–∞—Ç."""
        record_hash = record.get_hash()

        if record_hash in self.seen_hashes:
            self.duplicate_count += 1
            return None

        self.seen_hashes.add(record_hash)
        return record

# –¢–µ—Å—Ç–æ–≤—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ –∑ –¥—É–±–ª—ñ–∫–∞—Ç–∞–º–∏
test_users = [
    UserRecord('u1', 'john@example.com', 'John', 'USA'),
    UserRecord('u2', 'jane@example.com', 'Jane', 'UK'),
    UserRecord('u1', 'john@example.com', 'John', 'USA'),  # –î—É–±–ª—ñ–∫–∞—Ç
    UserRecord('u3', 'bob@example.com', 'Bob', 'USA'),
    UserRecord('u2', 'jane@example.com', 'Jane', 'UK'),  # –î—É–±–ª—ñ–∫–∞—Ç
]

print("\n1. –î–ï–î–£–ë–õ–Ü–ö–ê–¶–Ü–Ø:")
print("-" * 70)

dedup = DuplicateEliminator()
unique_users = []

for user in test_users:
    unique = dedup.add_record(user)
    if unique:
        unique_users.append(unique)

print(f"–í—Ö—ñ–¥–Ω—ñ –∑–∞–ø–∏—Å–∏: {len(test_users)}")
print(f"–£–Ω—ñ–∫–∞–ª—å–Ω—ñ: {len(unique_users)}")
print(f"–î—É–±–ª—ñ–∫–∞—Ç–∏ –≤–∏–¥–∞–ª–µ–Ω–æ: {dedup.duplicate_count}")

print("\n–£–Ω—ñ–∫–∞–ª—å–Ω—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ:")
for user in unique_users:
    print(f"  {user.user_id}: {user.name} ({user.email})")

print()

# ============================================================================
# 4. –ü–ê–†–ê–õ–ï–õ–¨–ù–ê –û–ë–†–û–ë–ö–ê (THREAD POOL)
# ============================================================================

print("\n" + "=" * 70)
print("PART 4: –ü–ê–†–ê–õ–ï–õ–¨–ù–ê –û–ë–†–û–ë–ö–ê - THREAD POOL")
print("=" * 70)

def process_heavy_task(item: Dict) -> Dict:
    """–Ü–º—ñ—Ç—É—î —Ç—è–∂–∫—É –æ–±—Ä–æ–±–∫—É (–Ω–∞–ø—Ä., API –≤–∏–∫–ª–∏–∫)."""
    time.sleep(0.01)  # –Ü–º—ñ—Ç–∞—Ü—ñ—è –∑–∞—Ç—Ä–∏–º–∫–∏
    return {
        'item_id': item['id'],
        'result': item['value'] * 2,
        'processed_at': time.time()
    }

items = [{'id': i, 'value': i * 10} for i in range(10)]

print("\n1. –ü–û–°–õ–Ü–î–û–í–ù–ê –û–ë–†–û–ë–ö–ê (–±–∞–∑–ª–∞–π–Ω):")
print("-" * 70)

start = time.time()
sequential_results = [process_heavy_task(item) for item in items]
sequential_time = time.time() - start

print(f"‚úÖ –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞ {len(items)} items: {sequential_time:.3f}s")

print("\n2. –ü–ê–†–ê–õ–ï–õ–¨–ù–ê –û–ë–†–û–ë–ö–ê (THREAD POOL):")
print("-" * 70)

start = time.time()
parallel_results = []

with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(process_heavy_task, item): item for item in items}

    for future in as_completed(futures):
        try:
            result = future.result()
            parallel_results.append(result)
        except Exception as e:
            logger.error(f"Task failed: {e}")

parallel_time = time.time() - start
speedup = sequential_time / parallel_time

print(f"‚úÖ –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ {len(items)} items: {parallel_time:.3f}s")
print(f"   Speedup: {speedup:.1f}x")

print()

# ============================================================================
# 5. INCREMENTAL AGGREGATION (–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞ –ª—å–æ—Ç—É)
# ============================================================================

print("\n" + "=" * 70)
print("PART 5: INCREMENTAL AGGREGATION - STATISTICS ON THE FLY")
print("=" * 70)

class IncrementalStats:
    """–û–±—á–∏—Å–ª—é—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–µ–∑ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤—Å—ñ—Ö –¥–∞–Ω–∏—Ö."""

    def __init__(self):
        self.count = 0
        self.sum = 0
        self.min = float('inf')
        self.max = float('-inf')
        self.sum_of_squares = 0

    def add(self, value: float) -> None:
        """–î–æ–¥–∞—î –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ –æ–Ω–æ–≤–ª—é—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        self.count += 1
        self.sum += value
        self.min = min(self.min, value)
        self.max = max(self.max, value)
        self.sum_of_squares += value ** 2

    def mean(self) -> float:
        """–°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è."""
        return self.sum / self.count if self.count > 0 else 0

    def variance(self) -> float:
        """–î–∏—Å–ø–µ—Ä—Å—ñ—è."""
        if self.count == 0:
            return 0
        return (self.sum_of_squares / self.count) - (self.mean() ** 2)

    def std_dev(self) -> float:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è."""
        return self.variance() ** 0.5

print("\n1. INCREMENTAL –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
print("-" * 70)

# –Ü–º—ñ—Ç—É—î–º–æ –ø–æ—Ç—ñ–∫ –∑–Ω–∞—á–µ–Ω—å
stream_values = [10, 20, 15, 30, 25, 22, 28, 18, 24, 26]

stats = IncrementalStats()

print(f"–ü–æ—Ç—ñ–∫ –∑–Ω–∞—á–µ–Ω—å: {stream_values}\n")

for value in stream_values:
    stats.add(value)

print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–µ–∑ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤—Å–∏—Ö –¥–∞–Ω–∏—Ö:")
print(f"  Count: {stats.count}")
print(f"  Min: {stats.min}")
print(f"  Max: {stats.max}")
print(f"  Mean: {stats.mean():.2f}")
print(f"  Std Dev: {stats.std_dev():.2f}")

print()

# ============================================================================
# 6. –¢–†–ê–ù–°–§–û–†–ú–ê–¶–Ü–Ø –î–ê–ù–ò–• (ETL PIPELINE)
# ============================================================================

print("\n" + "=" * 70)
print("PART 6: ETL PIPELINE - EXTRACT -> TRANSFORM -> LOAD")
print("=" * 70)

@dataclass
class TransformMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó."""
    extracted: int = 0
    transformed: int = 0
    loaded: int = 0
    errors: int = 0
    start_time: float = field(default_factory=time.time)
    end_time: float = 0

    def duration(self) -> float:
        return self.end_time - self.start_time if self.end_time > 0 else 0

class SimpleETLPipeline:
    """–ü—Ä–æ—Å—Ç–∏–π ETL pipeline."""

    def __init__(self):
        self.metrics = TransformMetrics()

    def extract(self, source_file: str) -> Generator[Dict, None, None]:
        """Extract: —á–∏—Ç–∞—î–º–æ –¥–∞–Ω—ñ."""
        with open(source_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    self.metrics.extracted += 1
                    yield data
                except json.JSONDecodeError:
                    self.metrics.errors += 1

    def transform(self, data: Dict) -> Optional[Dict]:
        """Transform: –æ—á–∏—â—É—î–º–æ —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ."""
        try:
            return {
                'id': int(data['id']),
                'value': float(data['value']) * 1.1,  # 10% markup
                'status': 'processed'
            }
        except (KeyError, ValueError):
            self.metrics.errors += 1
            return None

    def load(self, data: List[Dict], output_file: str) -> None:
        """Load: –∑–∞–ø–∏—Å—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏."""
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item) + '\n')
                self.metrics.loaded += 1

    def run(self, source_file: str, output_file: str) -> TransformMetrics:
        """–ó–∞–ø—É—Å–∫–∞—î ETL pipeline."""
        transformed_data = []

        for raw_data in self.extract(source_file):
            transformed = self.transform(raw_data)
            if transformed:
                transformed_data.append(transformed)
                self.metrics.transformed += 1

        self.load(transformed_data, output_file)
        self.metrics.end_time = time.time()

        return self.metrics

print("\n1. ETL PIPELINE:")
print("-" * 70)

pipeline = SimpleETLPipeline()
etl_metrics = pipeline.run(test_jsonl, "processed_events.jsonl")

print(f"‚úÖ ETL Pipeline –∑–∞–≤–µ—Ä—à–µ–Ω–∏–π")
print(f"  Extracted: {etl_metrics.extracted}")
print(f"  Transformed: {etl_metrics.transformed}")
print(f"  Loaded: {etl_metrics.loaded}")
print(f"  Errors: {etl_metrics.errors}")
print(f"  Duration: {etl_metrics.duration():.3f}s")

print()

# ============================================================================
# 7. –ü–†–ê–ö–¢–ò–ß–ù–Ü –ó–ê–í–î–ê–ù–ù–Ø
# ============================================================================

print("\n" + "=" * 70)
print("PART 7: –ü–†–ê–ö–¢–ò–ß–ù–Ü –ó–ê–í–î–ê–ù–ù–Ø")
print("=" * 70)

print("""
–ó–ê–í–î–ê–ù–ù–Ø 1 (–õ–ï–ì–ö–û): Streaming –ø—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ä—è–¥–∫—ñ–≤
  - –†–µ–∞–ª—ñ–∑—É–π—Ç–µ generator —â–æ —á–∏—Ç–∞—î CSV —Ñ–∞–π–ª
  - –ü–æ–≤–µ—Ä–Ω—ñ—Ç—å –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤
  - –ù–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–π—Ç–µ –≤–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º'—è—Ç—å

–ó–ê–í–î–ê–ù–ù–Ø 2 (–°–ï–†–ï–î–ù–¨–û): –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è —Ç–∞ –∞–≥—Ä–µ–≥–∞—Ü—ñ—è
  - –ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ JSON —Ñ–∞–π–ª
  - –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É–π—Ç–µ –∑–∞–ø–∏—Å–∏ –∑–∞ –∫—Ä–∏—Ç–µ—Ä—ñ—î–º (–Ω–∞–ø—Ä. value > 50)
  - –ê–≥—Ä–µ–≥—É–π—Ç–µ (sum, count, average)

–ó–ê–í–î–ê–ù–ù–Ø 3 (–°–ï–†–ï–î–ù–¨–û): –î–µ–¥—É–±–ª—ñ–∫–∞—Ü—ñ—è –∑–∞ –∫—ñ–ª—å–∫–æ–º–∞ –ø–æ–ª—è–º–∏
  - –†–µ–∞–ª—ñ–∑—É–π—Ç–µ –¥–µ–¥—É–±–ª—ñ–∫–∞—Ü—ñ—é –ø–æ composite key (id + email)
  - –†–æ–∑–ø–µ—á–∞—Ç–∞–π—Ç–µ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –≤–∏–¥–∞–ª–µ–Ω–æ
  - –ó–±–µ—Ä–µ–∂—ñ—Ç—å —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –∑–∞–ø–∏—Å–∏

–ó–ê–í–î–ê–ù–ù–Ø 4 (–°–ö–õ–ê–î–ù–û): –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –∑ –ª—ñ–º—ñ—Ç–æ–º
  - –†–µ–∞–ª—ñ–∑—É–π—Ç–µ ThreadPoolExecutor –¥–ª—è –æ–±—Ä–æ–±–∫–∏
  - –î–æ–¥–∞–π—Ç–µ –æ–±—Ä–æ–±–∫—É –ø–æ–º–∏–ª–æ–∫ (retry, fallback)
  - –í–∏–º—ñ—Ä—é–π—Ç–µ throughput

–ó–ê–í–î–ê–ù–ù–Ø 5 (–°–ö–õ–ê–î–ù–û): Multi-stage ETL pipeline
  - Extract –∑ CSV
  - Transform (–æ—á–∏—â–µ–Ω–Ω—è + –≤–∞–ª—ñ–¥–∞—Ü—ñ—è)
  - Load –≤ JSON –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏

–†–æ–∑–≤'—è–∂—ñ—Ç—å –∑–∞–≤–¥–∞–Ω–Ω—è –ø–µ—Ä–µ–¥ –¥–∏–≤—ñ—Ç—Ç—è–º —Ä—ñ—à–µ–Ω–Ω—è!
""")

print("\n" + "=" * 70)
print("–ò–¢–û–ì–ò")
print("=" * 70)

print("""
‚úÖ –©–æ –≤–∏ –¥—ñ–∑–Ω–∞–ª–∏—Å—è:
  1. Streaming –æ–±—Ä–æ–±–∫–∞ –¥–ª—è memory efficiency
  2. –ë–∞—Ç—á –æ–±—Ä–æ–±–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
  3. JSONL —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö —Ñ–∞–π–ª—ñ–≤
  4. –î–µ–¥—É–±–ª—ñ–∫–∞—Ü—ñ—è –∑ —Ö–µ—à—É–≤–∞–Ω–Ω—è–º
  5. –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –∑ ThreadPoolExecutor
  6. Incremental aggregation –±–µ–∑ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö
  7. ETL pipeline –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞

üîë Key patterns –¥–ª—è Senior Data Engineer:
  - Streaming vs. Batch: –∑–Ω–∞–π—Ç–∏ –±–∞–ª–∞–Ω—Å –∑–∞ memory vs. latency
  - –î–µ–¥—É–±–ª—ñ–∫–∞—Ü—ñ—è: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ hashing –¥–ª—è –º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–æ—Å—Ç—ñ
  - –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–º: ThreadPool –¥–ª—è I/O, multiprocessing –¥–ª—è CPU
  - –ú–µ—Ç—Ä–∏–∫–∏: –∑–∞–≤–∂–¥–∏ –≤–∏–º—ñ—Ä—é–π—Ç–µ throughput, latency, error rate
  - Graceful degradation: –æ–±—Ä–æ–±–ª—è–π—Ç–µ –ø–æ–º–∏–ª–∫–∏, –Ω–µ –∑—É–ø–∏–Ω—è–π—Ç–µ pipeline

‚ö†Ô∏è  –ß–∞—Å—Ç—ñ—ó –ø–æ–º–∏–ª–∫–∏:
  - –ó–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ –≤—Å–µ –≤ –ø–∞–º'—è—Ç—å (OOM for large files)
  - –ù–µ –æ–±—Ä–æ–±–ª—è—Ç–∏ –ø–æ–º–∏–ª–∫–∏ (–∑—É–ø–∏–Ω—è—î—Ç—å—Å—è –Ω–∞ –ø–µ—Ä—à—ñ–π –ø–æ–º–∏–ª—Ü—ñ)
  - –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è (–Ω–µ–º–æ–∂–ª–∏–≤–æ –¥–µ–±–∞“ë–∏—Ç–∏ –≤ production)
  - –ë–µ–∑ –º–µ—Ç—Ä–∏–∫ (–Ω–µ –∑–Ω–∞—î—Ç–µ –¥–µ –≤—É–∑—å–∫—ñ –º—ñ—Å—Ü—è)
  - –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –∑–∞–º—ñ—Å—Ç—å –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—ó (–º–∞—Ä–Ω—É—î—Ç—å—Å—è —á–∞—Å)

üöÄ –ù–∞—Å—Ç—É–ø–Ω–µ: Architecture patterns - design for scale
""")

# –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö —Ñ–∞–π–ª—ñ–≤
for file in [test_csv, test_jsonl, "processed_events.jsonl"]:
    if os.path.exists(file):
        os.remove(file)
