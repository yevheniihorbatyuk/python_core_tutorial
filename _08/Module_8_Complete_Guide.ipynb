{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 Complete Guide: Serialization and Object Copying\n",
    "\n",
    "This notebook covers the full Module 8 curriculum (beginner + advanced). It combines\n",
    "explanations, runnable examples, and practical checklists.\n",
    "\n",
    "Use this notebook as the main learning path, and jump into the scripts for deeper practice:\n",
    "- Beginner: `beginner_edition/*.py`\n",
    "- Advanced: `advanced_edition/*.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. Setup and environment check\n",
    "2. Part A (Beginner): Encapsulation and password hashing\n",
    "3. Part A (Beginner): Pickle basics\n",
    "4. Part A (Beginner): JSON serialization\n",
    "5. Part A (Beginner): CSV serialization\n",
    "6. Part A (Beginner): Shallow vs deep copy\n",
    "7. Part A (Beginner): Practice and mini projects\n",
    "8. Part B (Advanced): Dataclasses and validation\n",
    "9. Part B (Advanced): Production pickle patterns\n",
    "10. Part B (Advanced): Modern serialization (JSON Lines, MessagePack, Parquet)\n",
    "11. Part B (Advanced): Copying performance and optimization\n",
    "12. Part B (Advanced): Pydantic advanced validation\n",
    "13. Summary and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and environment check\n",
    "\n",
    "We keep most examples standard-library only. Advanced sections have optional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optional dependencies:\n",
      "  pydantic          -> available\n",
      "  pydantic_settings -> available\n",
      "  msgpack           -> available\n",
      "  pandas            -> available\n",
      "  pyarrow           -> available\n",
      "  polars            -> missing\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "\n",
    "optional = [\"pydantic\", \"pydantic_settings\", \"msgpack\", \"pandas\", \"pyarrow\", \"polars\"]\n",
    "availability = {name: importlib.util.find_spec(name) is not None for name in optional}\n",
    "\n",
    "print(\"Optional dependencies:\")\n",
    "for name, ok in availability.items():\n",
    "    print(f\"  {name:17} -> {'available' if ok else 'missing'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Beginner Edition\n",
    "\n",
    "Focus: solid fundamentals and safe defaults.\n",
    "\n",
    "Relevant scripts:\n",
    "- `beginner_edition/01_oop_encapsulation_basics.py`\n",
    "- `beginner_edition/02_pickle_basics.py`\n",
    "- `beginner_edition/03_json_csv_basics.py`\n",
    "- `beginner_edition/04_copying_basics.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encapsulation and password hashing\n",
    "\n",
    "Key ideas:\n",
    "- Keep sensitive values private (use name mangling or properties).\n",
    "- Never store raw passwords.\n",
    "- Use PBKDF2 and a unique salt per password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked password: ********\n",
      "Verify correct: True\n",
      "Verify wrong: False\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import hmac\n",
    "import os\n",
    "\n",
    "class User:\n",
    "    def __init__(self, email: str, password: str):\n",
    "        self.email = email\n",
    "        self._salt = os.urandom(16)\n",
    "        self._password_hash = self._hash_password(password)\n",
    "\n",
    "    def _hash_password(self, password: str) -> bytes:\n",
    "        return hashlib.pbkdf2_hmac(\"sha256\", password.encode(), self._salt, 100_000)\n",
    "\n",
    "    def verify_password(self, password: str) -> bool:\n",
    "        candidate = hashlib.pbkdf2_hmac(\"sha256\", password.encode(), self._salt, 100_000)\n",
    "        return hmac.compare_digest(candidate, self._password_hash)\n",
    "\n",
    "    @property\n",
    "    def password(self) -> str:\n",
    "        return \"********\"\n",
    "\n",
    "user = User(\"alice@example.com\", \"CorrectHorseBatteryStaple\")\n",
    "print(\"Masked password:\", user.password)\n",
    "print(\"Verify correct:\", user.verify_password(\"CorrectHorseBatteryStaple\"))\n",
    "print(\"Verify wrong:\", user.verify_password(\"wrong\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pickle basics (Python-only persistence)\n",
    "\n",
    "Pickle is powerful but unsafe for untrusted input.\n",
    "Use it only for trusted data under your control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: [Task(title='Learn pickle', done=False), Task(title='Practice copying', done=False)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from io import BytesIO\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    title: str\n",
    "    done: bool = False\n",
    "\n",
    "# Save to bytes\n",
    "buffer = BytesIO()\n",
    "pickle.dump([Task(\"Learn pickle\"), Task(\"Practice copying\")], buffer, protocol=5)\n",
    "\n",
    "# Load from bytes\n",
    "buffer.seek(0)\n",
    "loaded = pickle.load(buffer)\n",
    "print(\"Loaded:\", loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle safety notes\n",
    "\n",
    "- Never unpickle data from untrusted sources.\n",
    "- Prefer JSON for API input/output.\n",
    "- Use explicit versioning for long-lived pickles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom state with `__getstate__` and `__setstate__`\n",
    "\n",
    "Use these hooks to exclude sensitive fields or to support versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled state length: 105\n"
     ]
    }
   ],
   "source": [
    "class SecureToken:\n",
    "    def __init__(self, token: str):\n",
    "        self.token = token\n",
    "        self._cached = \"expensive-derivation\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"token\"] = \"***redacted***\"\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "obj = SecureToken(\"secret\")\n",
    "print(\"Pickled state length:\", len(pickle.dumps(obj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. JSON serialization (portable)\n",
    "\n",
    "JSON is safe and cross-language, but you must convert custom objects to dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Dana\",\n",
      "  \"address\": {\n",
      "    \"city\": \"Kyiv\",\n",
      "    \"country\": \"UA\"\n",
      "  }\n",
      "}\n",
      "Restored: Customer(name='Dana', address=Address(city='Kyiv', country='UA'))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "@dataclass\n",
    "class Address:\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "@dataclass\n",
    "class Customer:\n",
    "    name: str\n",
    "    address: Address\n",
    "\n",
    "customer = Customer(\"Dana\", Address(\"Kyiv\", \"UA\"))\n",
    "\n",
    "# Serialize using asdict\n",
    "json_str = json.dumps(asdict(customer), indent=2)\n",
    "print(json_str)\n",
    "\n",
    "# Restore manually\n",
    "loaded = json.loads(json_str)\n",
    "restored = Customer(loaded[\"name\"], Address(**loaded[\"address\"]))\n",
    "print(\"Restored:\", restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CSV serialization (tabular data)\n",
    "\n",
    "CSV is best for flat, tabular data. Use `csv.DictWriter` for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,score\n",
      "Alice,91\n",
      "Bob,84\n",
      "\n",
      "Loaded: [{'name': 'Alice', 'score': '91'}, {'name': 'Bob', 'score': '84'}]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "rows = [\n",
    "    {\"name\": \"Alice\", \"score\": 91},\n",
    "    {\"name\": \"Bob\", \"score\": 84},\n",
    "]\n",
    "\n",
    "out = StringIO()\n",
    "writer = csv.DictWriter(out, fieldnames=[\"name\", \"score\"])\n",
    "writer.writeheader()\n",
    "writer.writerows(rows)\n",
    "\n",
    "csv_text = out.getvalue()\n",
    "print(csv_text)\n",
    "\n",
    "# Read it back\n",
    "inp = StringIO(csv_text)\n",
    "reader = csv.DictReader(inp)\n",
    "loaded = [row for row in reader]\n",
    "print(\"Loaded:\", loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use JSON vs CSV\n",
    "\n",
    "- JSON: nested objects, APIs, config files\n",
    "- CSV: flat tables, spreadsheets, data exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Shallow vs deep copy\n",
    "\n",
    "Shallow copy duplicates the outer container but keeps inner references.\n",
    "Deep copy duplicates the entire structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original after shallow mutation: {'numbers': [1, 2, 3, 4], 'meta': {'owner': 'B'}}\n",
      "Deep copy remains: {'numbers': [1, 2, 3], 'meta': {'owner': 'A'}}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "original = {\"numbers\": [1, 2, 3], \"meta\": {\"owner\": \"A\"}}\n",
    "shallow = copy.copy(original)\n",
    "deep = copy.deepcopy(original)\n",
    "\n",
    "shallow[\"numbers\"].append(4)\n",
    "shallow[\"meta\"][\"owner\"] = \"B\"\n",
    "\n",
    "print(\"Original after shallow mutation:\", original)\n",
    "print(\"Deep copy remains:\", deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common pitfall: mutable default arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1, 1]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "def bad_accumulator(items=[]):\n",
    "    items.append(1)\n",
    "    return items\n",
    "\n",
    "print(bad_accumulator())\n",
    "print(bad_accumulator())\n",
    "\n",
    "\n",
    "def good_accumulator(items=None):\n",
    "    if items is None:\n",
    "        items = []\n",
    "    items.append(1)\n",
    "    return items\n",
    "\n",
    "print(good_accumulator())\n",
    "print(good_accumulator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practice and mini projects\n",
    "\n",
    "Practice files:\n",
    "- `beginner_edition/05_practice_tasks_beginner.py`\n",
    "- `beginner_edition/06_mini_projects_beginner.py`\n",
    "\n",
    "Suggested workflow:\n",
    "1. Run the scripts end-to-end.\n",
    "2. Modify inputs (add nested data, new fields).\n",
    "3. Re-run and compare outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Advanced Edition\n",
    "\n",
    "Focus: production patterns, validation, performance, and modern formats.\n",
    "\n",
    "Relevant scripts:\n",
    "- `advanced_edition/01_modern_encapsulation.py`\n",
    "- `advanced_edition/02_pickle_production.py`\n",
    "- `advanced_edition/03_modern_serialization.py`\n",
    "- `advanced_edition/04_copying_performance.py`\n",
    "- `advanced_edition/05_pydantic_dataclasses.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataclasses with validation (stdlib-first)\n",
    "\n",
    "You can validate after initialization and add computed properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSN: db.local:5432/app\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DatabaseConfig:\n",
    "    host: str\n",
    "    port: int\n",
    "    database: str\n",
    "    pool_size: int = 10\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not (1 <= self.port <= 65535):\n",
    "            raise ValueError(\"port out of range\")\n",
    "        if not self.database:\n",
    "            raise ValueError(\"database name required\")\n",
    "\n",
    "    @property\n",
    "    def dsn(self) -> str:\n",
    "        return f\"{self.host}:{self.port}/{self.database}\"\n",
    "\n",
    "cfg = DatabaseConfig(host=\"db.local\", port=5432, database=\"app\")\n",
    "print(\"DSN:\", cfg.dsn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production pickle patterns\n",
    "\n",
    "Use explicit versioning and avoid loading untrusted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized bytes: 99\n",
      "Loaded: ModelState(version=1, weights=[0.1, 0.2, 0.3])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelState:\n",
    "    version: int\n",
    "    weights: list[float]\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\"version\": self.version, \"weights\": self.weights}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if state.get(\"version\") != 1:\n",
    "            raise ValueError(\"Unsupported model version\")\n",
    "        self.version = state[\"version\"]\n",
    "        self.weights = state[\"weights\"]\n",
    "\n",
    "state = ModelState(version=1, weights=[0.1, 0.2, 0.3])\n",
    "blob = pickle.dumps(state)\n",
    "print(\"Serialized bytes:\", len(blob))\n",
    "print(\"Loaded:\", pickle.loads(blob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Modern serialization formats\n",
    "\n",
    "- JSON Lines is great for streaming logs.\n",
    "- MessagePack is compact binary JSON.\n",
    "- Parquet is columnar and efficient for analytics (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 1, \"value\": 10}\n",
      "{\"id\": 2, \"value\": 20}\n",
      "Loaded: [{'id': 1, 'value': 10}, {'id': 2, 'value': 20}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON Lines (one JSON object per line)\n",
    "records = [{\"id\": 1, \"value\": 10}, {\"id\": 2, \"value\": 20}]\n",
    "jsonl = \"\\n\".join(json.dumps(r) for r in records)\n",
    "print(jsonl)\n",
    "\n",
    "loaded = [json.loads(line) for line in jsonl.splitlines()]\n",
    "print(\"Loaded:\", loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MessagePack bytes: 19\n",
      "Unpacked: {'name': 'Alice', 'score': 99}\n"
     ]
    }
   ],
   "source": [
    "# Optional: MessagePack\n",
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec(\"msgpack\"):\n",
    "    import msgpack\n",
    "    packed = msgpack.packb({\"name\": \"Alice\", \"score\": 99}, use_bin_type=True)\n",
    "    unpacked = msgpack.unpackb(packed, raw=False)\n",
    "    print(\"MessagePack bytes:\", len(packed))\n",
    "    print(\"Unpacked:\", unpacked)\n",
    "else:\n",
    "    print(\"msgpack not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  score\n",
      "0   1     98\n",
      "1   2     87\n"
     ]
    }
   ],
   "source": [
    "# Optional: Parquet with pandas + pyarrow\n",
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec(\"pandas\") and importlib.util.find_spec(\"pyarrow\"):\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame([{\"id\": 1, \"score\": 98}, {\"id\": 2, \"score\": 87}])\n",
    "    path = \"_tmp_scores.parquet\"\n",
    "    df.to_parquet(path, index=False)\n",
    "    reloaded = pd.read_parquet(path)\n",
    "    print(reloaded)\n",
    "else:\n",
    "    print(\"pandas/pyarrow not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Copying performance and optimization\n",
    "\n",
    "Key idea: deep copy is expensive. Prefer immutability or copy-on-write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shallow copy (50x): 0.0001s\n",
      "Deep copy (5x):     0.2239s\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "nested = {\"items\": [list(range(1000)) for _ in range(200)]}\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(50):\n",
    "    copy.copy(nested)\n",
    "shallow_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(5):\n",
    "    copy.deepcopy(nested)\n",
    "deep_time = time.time() - start\n",
    "\n",
    "print(f\"Shallow copy (50x): {shallow_time:.4f}s\")\n",
    "print(f\"Deep copy (5x):     {deep_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization patterns\n",
    "\n",
    "- Prefer immutable records for shared data.\n",
    "- Use copy-on-write in dataframes where possible.\n",
    "- Avoid deep copies in tight loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Pydantic advanced validation (optional)\n",
    "\n",
    "If `pydantic` is installed, we can validate and normalize inputs at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_url='https://api.example.com' api_key='sk-1234567890abcdefghij'\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec(\"pydantic\"):\n",
    "    from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "    class APIConfig(BaseModel):\n",
    "        base_url: str = Field(...)\n",
    "        api_key: str = Field(..., min_length=20)\n",
    "\n",
    "        @field_validator(\"base_url\")\n",
    "        @classmethod\n",
    "        def validate_url(cls, v: str) -> str:\n",
    "            if not v.startswith((\"http://\", \"https://\")):\n",
    "                raise ValueError(\"base_url must start with http:// or https://\")\n",
    "            return v\n",
    "\n",
    "    cfg = APIConfig(base_url=\"https://api.example.com\", api_key=\"sk-1234567890abcdefghij\")\n",
    "    print(cfg)\n",
    "else:\n",
    "    print(\"pydantic not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_key='sk-test' debug=True workers=4\n"
     ]
    }
   ],
   "source": [
    "# Optional: Pydantic settings and env loading\n",
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec(\"pydantic\") and importlib.util.find_spec(\"pydantic_settings\"):\n",
    "    from pydantic_settings import BaseSettings\n",
    "    from pydantic import ConfigDict\n",
    "\n",
    "    class AppSettings(BaseSettings):\n",
    "        api_key: str = \"default\"\n",
    "        debug: bool = False\n",
    "        workers: int = 4\n",
    "\n",
    "        model_config = ConfigDict(env_file=\".env\")\n",
    "\n",
    "    settings = AppSettings(api_key=\"sk-test\", debug=True)\n",
    "    print(settings)\n",
    "else:\n",
    "    print(\"pydantic/pydantic_settings not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and next steps\n",
    "\n",
    "You now have both beginner and advanced coverage for Module 8.\n",
    "\n",
    "Recommended next actions:\n",
    "1. Run the beginner practice tasks and mini projects.\n",
    "2. Run the advanced scripts and compare performance outputs.\n",
    "3. Replace examples with your own data and re-run.\n",
    "\n",
    "Entry points:\n",
    "- Beginner: `beginner_edition/README_beginner.md`\n",
    "- Advanced: `advanced_edition/README_advanced.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
